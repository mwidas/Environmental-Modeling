---
title: "sensitivity_LHS_catm"
output: html_document
date: '2022-04-19'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sensitivity)
library(tidyverse)
library(lhs)
library(purrr)
```

# Example of using Latin Hypercube sampling for sensitivity analysis

Lets look at our compute atmospheric conductance example

```{r LHS}
# for formal sensitivity analysis it is useful to describe output in
# several summary statistics - how about mean, max and min yield
source("week3/R/Catm.R")


# set a random seed to make things 'random'
set.seed(1)

# which parameters
pnames = c("v", "height", "k_o", "k_d")

# how many parameters
npar =  length(pnames)
# how many samples
nsample = 100

# get hypercube quantiles
parm_quant = randomLHS(nsample, npar)

# set the column names to be = to the parameter names
colnames(parm_quant) = pnames

# choose distributions for parameters - this would come from
# what you know about the likely range of variation
# then use our random samples to pick the quantiles


parm = as.data.frame(matrix(nrow=nrow(parm_quant), ncol=ncol(parm_quant)))
colnames(parm) = pnames
# for each parameter pick samples 
# I'm using several examples normal distribution (with 10% standard deviation) and uniform with +- 10%
# in reality I should pick distribution from knowledge about uncertainty in parameters

# to make it easy to change i'm setting standard deviation / range variation to a variable

# using quantiles determined earlier apply to our data
# pvar is allowing us to take a 10% variation for the sd's
parm[,"v"] = qnorm(parm_quant[, "v"], mean=250, sd=30)
parm[,"height"] = qunif(parm_quant[,"height"], min=9.5, max=10.5)
parm[,"k_o"] = qnorm(parm_quant[,"k_o"], mean=0.1, sd= 0.01)
parm[,"k_d"] = qnorm(parm_quant[,"k_d"], mean=0.7, sd= 0.01)

head(parm)
```

# Run model for parameter sets

* We will do this in R

* We can use **pmap** to efficiently run our model for all of our parameter sets

but first

# Examining Output

Sensitivity of What?

If your model is estimating a single value, you are done

* long term mean almond yield anomoly
*  mean profit from solar

But models are often estimating multiple values

* streamflow
* almond yield anomoly for multiple years
  
In that case to quantify sensitivity you need summary metrics

* mean
* max
* min
* variance

Which one depends on what you care about


```{r almondsens}
# lets now run our model for all of the parameters generated by LHS
# pmap is useful here - it is a map function that uses the actual names of input parameters

conductance = parm %>% pmap(Catm)

# notice that what pmap returns is a list 
head(conductance)

# turn results in to a dataframe for easy display/analysis
conductance = unlist(conductance)

conductance = cbind.data.frame(parm, conductance=conductance)

conductance
```


#  Plotting 

Plot relationship between parameter and output
to understand how uncertainty in parameter impacts the output to determine over what ranges of the parameter uncertainty is most important (biggest effect)


* Use a box plot (of output)
to graphically show the impact of uncertainty on output of interest

* To see more of the distribution - graph the cumulative distribution
  * high slope, many values in that range
  * low slope, few values in that range
  * constant slope, even distribution
  
* Scatterplots against parameter values

---

```{r senplot}

# add uncertainty bounds on our estimates
tmp = conductance %>% gather(value="value", key="conductance")
# boxplot show minimum yield is more sensitive as boxplot is wider
ggplot(tmp, aes(conductance, value, col=conductance))+geom_boxplot()+
  labs(y="Conductance")


# note that you don't see the ranges because of the scale (min yield anomoly much smaller than max) - here's a more informative way to graph
ggplot(tmp, aes(conductance, value, col=conductance))+
  geom_boxplot()+labs(y="Conductance")+
  facet_wrap(~conductance, scales="free" )


# cumulative distribution
ggplot(conductance, aes(conductance))+stat_ecdf()


# plot parameter sensitivity
# a bit tricky but nice way to make it easy to plot all parameters against all values
tmp = cbind.data.frame(conductance, parm)
tmp2 = tmp %>% gather(conductance, value="yvalue",key="conductancetype")
tmp3 = tmp2 %>% gather(-yvalue, -conductancetype, key="parm", value="parmvalue")
ggplot(tmp3, aes(parmvalue, yvalue, col=conductancetype))+geom_point()+facet_wrap(~conductancetype*parm, scales="free", ncol=5)

```

Quantifying

# Correlation and Pcc for Maximum Yield
```{r quantifying}
# combine parameter sets with output


# simple correlation coefficients
result = map(parm, cor.test, y=yieldsd$maxyield)
result$Tmincoeff1
result$Tmincoeff2
result$Pcoeff1
result$Pcoeff2
result$intercep

# just the confidence interval
justconf = result %>% map_df("conf.int")
justconf

# partial regression rank coefficients

senresult_rank = pcc(parm, yieldsd$maxyield, rank=TRUE )
senresult_rank
plot(senresult_rank)
```

# Correlation and Pcc for Minimum Yield
```{r quantifying}
# combine parameter sets with output


# simple correlation coefficients
result_min = map(parm, cor.test, y=yieldsd$minyield)
result_min$Tmincoeff1
result_min$Tmincoeff2
result_min$Pcoeff1
result_min$Pcoeff2
result_min$intercep

# just the confidence interval
justconf_min = result_min %>% map_df("conf.int")
justconf_min

# partial regression rank coefficients

senresult_rank_min = pcc(parm, yieldsd$minyield, rank=TRUE )
senresult_rank_min
plot(senresult_rank_min)

```




